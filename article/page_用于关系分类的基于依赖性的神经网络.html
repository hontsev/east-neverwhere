<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>乌有乡 - East Neverwhere</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
	<link href="../css/sixsquarestyle.css" rel="stylesheet">
	<link href="../css/mystyle.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="//cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="//cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body style="background-color:#000;background-image:url(../image/bg.jpg);background-repeat:no-repeat; background-attachment:fixed;background-size:cover">
  <nav class="navbar navbar-inverse" role="navigation">
   <div class="navbar-header">
      <a class="navbar-brand" href="http://eastneverwhere.party/">乌有乡 - East Neverwhere</a>
   </div>
   <div>
      <ul class="nav navbar-nav" id='menubar'></ul>
   </div>
</nav>
      <div class="container">
	  <div class="col-md-12" style="padding-bottom:60px">
			<h2 class="form-signin-heading" style="color:#fff"><b>用于关系分类的基于依赖性的神经网络</b></h2>
		</div>		
	  <div class="col-md-12">
	  	<div class="col-md-12 text pull-right" >
			<p>用于关系分类的基于依赖性的神经网络</p><p>Proceedings&nbsp;of&nbsp;the&nbsp;53rd&nbsp;Annual&nbsp;Meeting&nbsp;of&nbsp;the&nbsp;Association&nbsp;for&nbsp;Computational&nbsp;Linguistics&nbsp;and&nbsp;the&nbsp;7th&nbsp;International&nbsp;Joint&nbsp;Conference&nbsp;on&nbsp;Natural&nbsp;Language&nbsp;Processing（Short&nbsp;Papers），第285-290页，中国北京，2015年7月26&nbsp;&nbsp;-&nbsp;&nbsp;31日。</p><p>c&nbsp;2015计算语言学协会基于依赖关系分类的神经网络杨柳1,2&nbsp;*古鲁伟3&nbsp;Sujian&nbsp;Li1,2&nbsp;Heng&nbsp;Ji4&nbsp;Ming&nbsp;Zhou3&nbsp;Houfeng&nbsp;Wang1,2&nbsp;1中国科学技术大学北京大学计算语言学科学实验室2中国科学技术创新中心语言能力，中国江苏徐州3M&nbsp;Microsoft&nbsp;Research，北京，中国4计算机科学系，伦斯勒理工学院，特洛伊，纽约州，美国{cs-ly，lisujian，wanghf&nbsp;}@pku.edu.cn&nbsp;{furu，mingzhou}&nbsp;@&nbsp;microsoft。&nbsp;com&nbsp;jih@rpi.edu摘要以前的关系分类研究已经验证了使用依赖性最短路径或子树的有效性。</p><p>在本文中，我们进一步探讨如何充分利用这些依赖信息的组合。</p><p>我们首先提出一种称为增强相关路径（ADP）的新结构，它由两个实体之间的最短相关路径和附加到最短路径的子树组成。</p><p>为了利用ADP结构背后的语义表示，我们开发了基于依赖的神经网络（DepNN）：设计用于对子树进行建模的递归神经网络，以及用于捕获最短路径上最重要特征的卷积神经网络。</p><p>对SemEval-2010数据集的实验表明，我们提出的方法实现了最先进的结果。</p><p>1引言关系分类旨在对句子中两个实体之间的语义关系进行分类。</p><p>它在从非结构化文本的强大的知识提取中起着至关重要的作用，并且在各种自然语言处理应用中作为中间步骤。</p><p>大多数现有方法遵循基于机器学习的框架，并专注于设计有效特征以获得更好的分类性能。</p><p>在先前的方法中已经报告了在实体之间使用依赖关系的关系分类的有效性（Bach和Badaskar，2007）。</p><p>例如，Suchanek&nbsp;et&nbsp;al。</p><p>（2006）仔细选择了一套来自标记化和依赖解析的功能，并扩展了其中的一些功能以生成高阶特征*在微软研究实习期间的贡献。</p><p>以不同的方式。</p><p>Culotta和Sorensen（2004）设计了一个依赖树内核，并附加了更多的信息，包括树中每个节点的部分语音标记，分块标记。</p><p>有趣的是，Bunescu和Mooney（2005）提供了一个重要的见解，即依赖图中两个实体之间的最短路径集中了大部分信息来识别它们之间的关系。</p><p>Nguyen&nbsp;et&nbsp;al。</p><p>（2007）通过在提取前关键词的指导下分析多个子树来开发这些想法。</p><p>以前的工作表明，关系分类中最有用的依赖信息包括最短的依赖路径和依赖子树。</p><p>这两种信息提供不同的功能，它们的协作可以提高关系分类的性能（详见第二章）。</p><p>然而，如何统一有效地组合这两个组件仍然是一个开放的问题。</p><p>在本文中，我们提出一个名为增强依赖路径（ADP）的新型结构，它将依赖子树附加到最短依赖路径上的单词，并专注于探索ADP结构后面的语义表示。</p><p>最近，深度学习技术已经广泛用于探索复杂结构后面的语义表示。</p><p>这为我们提供了在神经网络框架中建模ADP结构的机会。</p><p>因此，我们提出了一个基于依赖的框架，其中两个神经网络用于分别建模最短相关性路径和依赖性子树。</p><p>一个卷积神经网络（CNN）被应用在最短相关路径上，因为CNN适合捕获平面结构中最有用的特征。</p><p>递归神经网络（RNN）用于从依赖子树提取语义表示，因为RNN擅于建模分层结构。</p><p>要连接这两个网络，每个词在最短的285一个窃贼试图偷取卡车用螺丝刀打破了点火。</p><p>在安息日，祭司们用祭司的工作打破了诫命。</p><p>检测准备与准备S1：S2：图1：句子及其依赖树。</p><p>（a）在S1中增强的相关路径。</p><p>（b）增强的依赖路径在S2中。</p><p>图2：增强的依赖关系路径。</p><p>路径与从其子树生成的表示相结合，加强最短路径的语义表示。</p><p>以这种方式，增强的相关性路径被表示为可以进一步用于关系分类的连续语义向量。</p><p>2问题定义和动机关系分类的任务可以定义如下。</p><p>给定具有注释的一对实体e1和e2的句子S，任务是根据一组预定义的关系类（例如，内容&nbsp;-&nbsp;容器，因果效应）来识别e1和e2之间的语义关系。</p><p>例如，在图2中，两个实体e1&nbsp;=小偷和e2&nbsp;=螺丝刀之间的关系是InstrumentAgency。</p><p>Bunescu和Mooney（2005）首先使用两个实体之间的最短相关路径来捕获谓词参数序列（例如，图2中的“thief←broke→screwdriver”），这为关系分类提供了强有力的证据。</p><p>如我们所观察到的，最短路径包含更多信息，并且附加到最短路径上的每个节点的子树未被充分利用。</p><p>例如，图2a和2b示出了具有类似的最短依赖路径但属于不同关系类的两个实例。</p><p>在这种情况下，仅使用路径的方法将失败。</p><p>然而，我们可以通过附加的子树例如“dobj→诫命”和“dobj→点火”来区分这两个路径。</p><p>基于这样的许多观察，我们提出了组合子树和最短路径以形成用于分类关系的更精确结构的想法。</p><p>这种组合结构被称为“增强的相关性路径（ADP）”，如图2所示。</p><p>接下来，我们的目标是捕获两个实体之间的ADP结构的语义表示。</p><p>我们首先采用递归神经网络根据其附加的依赖子树对每个词进行建模。</p><p>基于每个词的语义信息，我们设计一个卷积神经网络，以获得最短的相关路径上的显着语义特征。</p><p>3基于依赖的神经网络在本节中，我们将介绍如何使用神经网络技术和依赖信息来探索两个实体之间的语义连接。</p><p>我们将我们的建模ADP结构作为基于依赖的神经网络（DepNN）。</p><p>图3说明了具体示例的DepNN。</p><p>首先，我们将每个词w和依赖关系r与向量表示xw，xr∈R&nbsp;dim相关联。</p><p>对于在最短相关性路径上的每个单词w，我们从其叶词到根生成RNN以生成子树嵌入cw并且连接cw与xw以用作w的最终表示。</p><p>接下来，CNN被设计为基于其字和关系的表示来对最短相关性路径建模。</p><p>最后，我们的框架可以有效地表示两个实体之间的语义连接，同时考虑更全面的依赖性信息。</p><p>3.1建模依赖子树对依赖子树建模的目的是为最短路径上的字找到一个适当的表示。</p><p>我们假设每个单词w可以由它自己和它的孩子在依赖子树上解释。</p><p>然后，对于子树上的每个词w，其词嵌入xw∈R&nbsp;dim和子树表示cw∈R&nbsp;dimc被连接以形成其最终表示pw∈R&nbsp;dim&nbsp;dimc。</p><p>对于没有子树的单词，我们将其子树表示设置为cLEAF。</p><p>字的子树表示通过变换其子字的表示来导出。</p><p>286&nbsp;Wdet&nbsp;priests&nbsp;nsubj&nbsp;broke&nbsp;prep_with&nbsp;work&nbsp;the&nbsp;commandament祭司的安息日Max&nbsp;Over&nbsp;Time&nbsp;Subtree嵌入窗口处理递归神经网络卷积神经网络最短路径Wdet&nbsp;Wdobj&nbsp;Wamod&nbsp;W1字嵌入子树嵌入Wprep-on&nbsp;Wdet图3：基于依赖的神经网络。</p><p>在子树的自下而上构造期间，每个词与依赖关系相关联，例如图3中的dobj。</p><p>对于每个依赖关系r，我们设置在训练期间学习的变换矩阵Wr∈R&nbsp;dimc×（dim&nbsp;dimc）。</p><p>然后我们可以得到，其中R（w，q）表示依赖关系，其中R（w，q）表示依赖关系（w，q）在词w和其子词q之间。</p><p>该过程递归地继续到最短路径上的根字。</p><p>3.2建模最短依赖路径为了对两个实体之间的语义关系进行分类，我们进一步探讨了它们的最短相关路径后面的语义表示，它可以被看作是一个字序列和依赖关系，如图2中的粗体部分所示。</p><p>由于卷积神经网络（CNN）擅于捕获来自一系列对象的显着特征，我们设计一个CNN来处理最短的相关路径。</p><p>CNN包含对象表示窗口上的卷积运算，随后是池操作。</p><p>我们知道，通过对子树建模，最短路径上的词w与表示pw相关联。</p><p>对于在最短路径上的依赖关系r，我们将其表示设置为向量xr∈R&nbsp;dim。</p><p>当在序列上应用滑动窗口时，我们将窗口大小设置为k。</p><p>例如，当k&nbsp;=&nbsp;3时，具有n个字的最短相关路径的滑动窗口是：{[rs&nbsp;w1&nbsp;r1]，[r1&nbsp;w2&nbsp;r2]</p><p>。</p><p>。</p><p>，[rn-1&nbsp;wn&nbsp;re]}其中rs和re用于表示两个实体之间的最短依赖关系路径的开始和结束。</p><p>我们将k个邻近字（或依赖关系）表示连接到一个新的向量中。</p><p>假设Xi∈R&nbsp;dim·k&nbsp;dimc·nw作为第i个窗口的连接表示，其中nw是一个窗口中的字数。</p><p>卷积运算涉及滤波器W&nbsp;1∈Rl×（dim·k&nbsp;dimc·nw），其对Xi进行操作以产生具有l维的新特征向量Li，其中为了简单，忽略偏置项。</p><p>然后将W1应用于最短相关性路径中的每个可能窗口，以产生特征图：[L0，L1，L2，...]。</p><p>接下来，我们采用广泛使用的最大时间合并操作（Collobert等人，2011），其可以保留最重要的特征，以从特征图获得最终表示L.</p><p>也就是说，L&nbsp;=&nbsp;max（L0，L1，L2，</p><p>。</p><p>。）。</p><p>3.3学习与其他关系分类系统一样，我们还引入了一些词汇级别的特征，例如命名实体标签和WordNet超词，这证明对这个任务很有用。</p><p>我们将它们与ADP表示L连接以产生组合矢量M.</p><p>然后我们将M传递给完全连接的sof&nbsp;tmax层，其输出是关于标签的概率分布y。</p><p>然后，优化目标是最小化地面真实标签向量和sof&nbsp;tmax输出之间的交叉熵误差。</p><p>使用反向传播法学习参数（Rumelhart等人，1988）。</p><p>4实验我们比较了SemEval-2010数据集上的多个基线的DepNN（Hendrickx&nbsp;et&nbsp;al。，2010）。</p><p>训练集包括8000个句子，并且测试集包括2717个句子。</p><p>有9&nbsp;287种关系类型，每种类型有两个方向。</p><p>不属于这些类别的实例标记为“其他”。</p><p>官方评估指标是宏观平均F1分数（不包括其他），并考虑方向。</p><p>我们使用由Stanford&nbsp;Parser（Klein和Manning，2003）生成的依赖树和collapsed选项。</p><p>4.1不同组件的贡献我们首先展示DepNN的不同组件的贡献。</p><p>在实验中使用了两种不同类型的用于初始化的字嵌入。</p><p>一个是由SENNA提供的50-d嵌入（Collobert等人，2011）。</p><p>第二个是用于（Yu等人，2014）的200-d嵌入，使用word2vec1在Gigaword上训练。</p><p>所有超参数设置为5倍交叉验证。</p><p>模型F1&nbsp;50-d&nbsp;200-d基线（路径词）73.8&nbsp;75.5依赖关系80.3&nbsp;81.8附加子树81.2&nbsp;82.8词法特征82.7&nbsp;83.6表1：具有不同组分的DepNN的性能。</p><p>我们从使用CNN只有最短路径上的词的基线模型开始。</p><p>然后我们添加依赖关系和附加的子树。</p><p>结果表明，两个部分对关系分类都有效。</p><p>嵌入在依赖关系和子树中的丰富的语言信息一方面可以帮助区分相同单词的不同功能，另一方面可以推断未知单词在句子中的作用。</p><p>最后，增加了词法特征，DepNN实现了最先进的结果。</p><p>4.2与基线比较在本小节中，我们将DepNN与几种基线关系分类方法进行比较。</p><p>这里，DepNN和基线都是基于由于较大的语料库和较高的维度在Gigaword上训练的200-d嵌入。</p><p>SVM（Rink和Harabagiu，2010）：这是在SemEval-2010中执行得最好的系统。</p><p>它利用许多外部语料库从句子中提取特征以构建SVM分类器。</p><p>SVM&nbsp;POS，PropBank，形态词汇，TextRunner，FrameNet&nbsp;82.2依赖解析等。</p><p>MV-RNN&nbsp;POS，NER，WordNet&nbsp;81.82&nbsp;CNN&nbsp;WordNet&nbsp;82.7&nbsp;FCM&nbsp;NER&nbsp;83.0&nbsp;DT-RNN&nbsp;NER&nbsp;73.1&nbsp;DepNN&nbsp;WordNet&nbsp;83.0&nbsp;NER&nbsp;83.6表2：使用Gigaword嵌入的SemEval-2010数据集的结果。</p><p>MV-RNN（Socher等人，2012）：该模型找到组成分析树中的两个实体之间的路径，然后利用每个单词的矩阵来学习其最高节点的分布式表示，以使组成具体化。</p><p>CNN：Zeng&nbsp;et&nbsp;al。</p><p>（2014）建立了一个句子的标记的卷积模型来学习句子级特征向量。</p><p>它使用一个特殊的位置矢量，表示当前输入字与两个标记实体的相对距离。</p><p>FCM（Yu&nbsp;et&nbsp;al。，2014）：FCM将句子分解为子结构，并为每个子结构提取特征，形成子结构嵌入。</p><p>这些嵌入通过sumpooling组合并输入到sof&nbsp;tmax分类器中。</p><p>DT-RNN（Socher&nbsp;et&nbsp;al。，2014）：这是一个用于建模依赖树的RNN。</p><p>它通过线性组合结合节点的字嵌入与它的孩子，但不是子树嵌入。</p><p>我们将增强的依赖性路径适配为依赖性子树并应用DT-RNN。</p><p>如表2所示，DepNN使用NER特征实现最佳结果（83.6）。</p><p>WordNet功能还可以提高DepNN的性能，但不如NER那么明显。</p><p>Yu&nbsp;et&nbsp;al。</p><p>（2014）有类似的观察，因为更大数量的WordNet标签可能会导致过度拟合。</p><p>SVM实现了可比较的结果，虽然特征工程的质量高度依赖于人类经验和外部NLP资源。</p><p>MV-RNN使用递归过程对构成分析树进行建模，其F1分数比DepNN低大约1.8％。</p><p>同时，MVR-NN训练非常慢，因为每个词与矩阵相关联。</p><p>CNN和FCM使用来自整个句子的特征并且实现类似的性能。</p><p>DT-RNN是所有基线中最差的，尽管在原始文件中报道的2MV-RNN在SENNA嵌入上获得较高的F1分数（82.7）。</p><p>288还考虑来自最短相关路径和附加子树的信息。</p><p>在我们分析时，最短依赖路径和子树在关系分类中扮演不同的角色。</p><p>然而，我们可以看到DT-RNN不区分最短路径和子树的建模过程。</p><p>这种现象也出现在基于内核的方法中（Wang，2008），其中树内核执行的比最短路径内核差。</p><p>我们还研究了DepNN模型，发现它可以识别不同模式的词和依赖关系。</p><p>例如，在仪器&nbsp;-&nbsp;机构关系中，发现“使用”和依赖关系“prep&nbsp;with”起主要作用。</p><p>5结论在本文中，我们建议通过在神经网络框架中建模增强的依赖路径来对实体之间的关系进行分类。</p><p>我们提出一种新颖的方法，DepNN，利用卷积神经网络和递归神经网络的优势来建模这种结构。</p><p>实验结果表明，DepNN实现了最先进的性能。</p><p>致谢我们感谢所有匿名审阅者的有见地的意见。</p><p>这项工作得到了中国国家重点基础研究计划（No.</p><p>中国国家自然科学基金（No.CB340504）。</p><p>61273278和61370117），中国国家社会科学基金（No：12</p><p>本文的通信作者是李素建。</p><p>参考文献Nguyen&nbsp;Bach和Sameer&nbsp;Badaskar。</p><p>2007年。</p><p>关系提取的调查。</p><p>语言技术学院，卡内基梅隆大学。</p><p>拉兹万</p><p>Bunescu和Raymond&nbsp;J.</p><p>门尼。</p><p>2005年。</p><p>关系提取的最短路径依赖内核。</p><p>在计算语言学协会的北美分会。</p><p>Ronan&nbsp;Collobert，Jason&nbsp;Weston，Leon&nbsp;Bottou，Michael'Karlen，Koray&nbsp;Kavukcuoglu和Pavel&nbsp;Kuksa。</p><p>2011年。</p><p>自然语言处理（几乎）从头开始。</p><p>机器学习研究杂志，12：2493-2537。</p><p>Aron&nbsp;Culotta和Jeffrey&nbsp;S.</p><p>索伦森。</p><p>2004年。</p><p>关系提取的依赖关系树内核。</p><p>在计算语言学协会会议，第423-429页。</p><p>Iris&nbsp;Hendrickx，Zornitsa&nbsp;Kozareva，Preslav&nbsp;Nakov，Sebastian&nbsp;Pad&nbsp;ok，Marco&nbsp;Pennacchiotti，Lorenza&nbsp;Romano和Stan&nbsp;Szpakowicz。</p><p>2010。</p><p>SemEval-2010任务8：对数名词之间的语义关系的多向分类。</p><p>Dan&nbsp;Klein和Christopher&nbsp;D.</p><p>曼宁。</p><p>2003年。</p><p>准确的非灵活解析。</p><p>在计算语言学协会会议，第423-430页。</p><p>Dat&nbsp;PT&nbsp;Nguyen，Yutaka&nbsp;Matsuo和Mitsuru&nbsp;Ishizuka。</p><p>2007年。</p><p>使用子树挖掘从维基百科中提取关系。</p><p>在全国人工智能会议记录，第22卷，第1414页。</p><p>Menlo&nbsp;Park，CA;&nbsp;Cambridge，MA;伦敦;&nbsp;AAAI&nbsp;Press;&nbsp;MIT&nbsp;Press;&nbsp;1999年。</p><p>Bryan溜冰场和Sanda&nbsp;Harabagiu。</p><p>2010。</p><p>Utd：通过组合词汇和语义资源来分类语义关系。</p><p>在第五届国际语义评估研讨会，第256-259页，乌普萨拉，瑞典，7月。</p><p>计算语言学协会。</p><p>David&nbsp;E&nbsp;Rumelhart，Geoffrey&nbsp;E&nbsp;Hinton和Ronald&nbsp;J&nbsp;Williams。</p><p>1988。</p><p>通过反向传播错误学习表示法。</p><p>认知建模。</p><p>理查德·希克，布罗迪·瓦格尔</p><p>Manning和Andrew&nbsp;Y.</p><p>Ng。</p><p>2012年。</p><p>通过递归矩阵向量空间的语义组合性。</p><p>在2012年联合会议自然语言处理和计算自然语言学习的经验方法论文集，第1201-1211页，济州岛，韩国，7月。</p><p>计算语言学协会。</p><p>Richard&nbsp;Socher，Andrej&nbsp;Karpathy，Quoc&nbsp;V&nbsp;Le，Christopher&nbsp;D&nbsp;Manning和Andrew&nbsp;Y&nbsp;Ng。</p><p>2014。</p><p>接地组成语义，用于查找和描述带有句子的图像。</p><p>计算语言学协会学报，2：207-218。</p><p>Fabian&nbsp;M&nbsp;Suchanek，Georgiana&nbsp;Ifrim和Gerhard&nbsp;Weikum。</p><p>2006年。</p><p>结合语言和统计分析来从Web文档中提取关系。</p><p>在Proceedings&nbsp;of&nbsp;the&nbsp;12th&nbsp;ACM&nbsp;SIGKDD&nbsp;international&nbsp;conference&nbsp;on&nbsp;Knowledge&nbsp;discovery&nbsp;and&nbsp;data&nbsp;mining，pages&nbsp;712-717。</p><p>ACM。</p><p>王梦琼。</p><p>2008年。</p><p>关系提取的依赖性路径内核的重新检查。</p><p>在IJCNLP，第841-846页。</p><p>Mo&nbsp;Yu，Matthew&nbsp;Gormley和Mark&nbsp;Dredze。</p><p>2014。</p><p>基于因子的组合嵌入模型。</p><p>在NIPS的学习语义学讲座。</p><p>刘建增，刘康，周伟，周光友，赵俊。</p><p>2014。</p><p>通过卷积深度神经网络的关系分类。</p><p>在COLING&nbsp;2014，第25届国际计算语言学会议：技术论文，第2335-2344页，都柏林，爱尔兰，8月。</p><p>都柏林城市大学和计算语言学协会。</p><p>290。</p><p>&nbsp;</p>
		</div>
		
	  </div>
	  

    </div> <!-- /container -->
    <script src="/js/jquery-1.12.0.min.js"></script>
	<script src="/js/jquery.goup.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
	<script src="/js/menubar.js"></script>
	<script>
	 $(document).ready(function () {
 
            $.goup({
                trigger: 100,
                bottomOffset: 150,
                locationOffset: 100,
            });
 
        });
	</script>
	
  </body>
</html>