<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>乌有乡 - East Neverwhere</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
	<link href="../css/sixsquarestyle.css" rel="stylesheet">
	<link href="../css/mystyle.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="//cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="//cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body style="background-color:#000;background-image:url(../image/bg.jpg);background-repeat:no-repeat; background-attachment:fixed;background-size:cover">
  <nav class="navbar navbar-inverse" role="navigation">
   <div class="navbar-header">
      <a class="navbar-brand" href="/">乌有乡 - East Neverwhere</a>
   </div>
   <div>
      <ul class="nav navbar-nav" id='menubar'></ul>
   </div>
</nav>
      <div class="container">
	  <div class="col-md-12" style="padding-bottom:60px">
			<h2 class="form-signin-heading" style="color:#fff"><b>神经会话模型</b></h2>
		</div>		
	  <div class="col-md-12">
	  	<div class="col-md-12 text pull-right" >
			<p>神经会话模型</p><p>摘要</p><p>会话建模是自然语言理解和机器智能的重要任务。</p><p>虽然存在先前的方法，但是它们通常限于特定的域（例如，预订机票）并且需要手工制作的规则。</p><p>在本文中，我们提出一个简单的方法，这个任务使用最近提出的序列序列框架。</p><p>我们的模型通过预测给定上一句或下一句话中的句子的下一句来进行对话。</p><p>我们的模型的优点是它可以端到端地进行训练，因此需要少得多的手工制定的规则。</p><p>我们发现，这个简单的模型可以生成简单的对话给定一个大的会话训练数据集。</p><p>我们的初步结果表明，尽管优化了错误的目标函数，该模型能够很好地交谈。</p><p>它能够从领域特定数据集以及电影字幕的大型，嘈杂和一般领域数据集中提取知识。</p><p>在特定于域的IT帮助台数据集上，模型可以通过对话找到解决技术问题的方案。</p><p>在嘈杂的开放域电影抄本数据集上，模型可以执行简单形式的常识推理。</p><p>正如所料，我们还发现缺乏一致性是我们模型的常见失效模式。</p><p>1。引言</p><p>神经网络的端到端训练的进步已经导致在诸如语音识别，计算机视觉和语言处理的许多领域中的显着进步。</p><p>最近的工作表明神经网络可以做更多而不仅仅是分类，它们可以用于将复杂结构映射到其它复杂结构。</p><p>这方面的一个例子是将序列映射到在自然语言理解中具有直接应用的另一序列的任务（Sutskever等人，2014）。</p><p>这个框架的主要优点是它需要很少的特征工程和领域特异性，同时匹配或超越最先进的结果。</p><p>这种进步，在我们看来，允许研究人员工作的领域知识可能不容易获得的任务，或任务，太难以手动设计规则。</p><p>对话建模可以直接受益于这个公式，因为它需要查询和响应之间的映射。</p><p>由于这种映射的复杂性，会话建模以前被设计为在域中非常狭窄，具有对特征工程的主要承诺。</p><p>在这项工作中，我们实验对话建模任务，通过将其转化为预测下一个序列给定前一个序列或序列使用复发网络的任务（Sutskever等，2014）。</p><p>我们发现，这种方法可以令人惊讶地产生流畅和准确的对话回复。</p><p>我们从谈话的IT帮助台数据集中的聊天会话测试模型，并发现模型有时可以跟踪问题，并为用户提供有用的答案。</p><p>我们还对从电影字幕的嘈杂数据集获得的对话进行实验，并发现该模型可以进行自然对话，有时执行简单形式的常识推理。</p><p>在这两种情况下，与n元模型相比，循环网络获得更好的困惑，并捕获重要的长程相关性。</p><p>从定性的角度来看，我们的模型有时能够产生自然的对话。</p><p>2。相关工作</p><p>我们的方法是基于最近的工作，建议使用神经网络将序列映射到序列（Kalchbrenner</p><p>这个框架已经是用于神经机器翻译的神经对话模型，并实现了来自WMT'14数据集（Luong等，2014;&nbsp;Jean等，2014）的英语&nbsp;-&nbsp;法语和英语&nbsp;-&nbsp;德语翻译任务的改进。</p><p>它也被用于其他任务，如解析（Vinyals等人，2014a）和图像字幕（Vinyals等人，2014b）。</p><p>由于众所周知，香草RNNs遭受渐变的消失，大多数研究人员使用长期短期记忆（LSTM）循环神经网络的变体（Hochreiter</p><p>我们的工作也受到最近成功的神经语言建模的启发（Bengio&nbsp;et&nbsp;al。，2003;&nbsp;Mikolov&nbsp;et&nbsp;al。，2010;&nbsp;Mikolov，2012），这表明循环神经网络是自然语言的相当有效的模型。</p><p>最近，Sordoni等人的工作</p><p>（Sordoni&nbsp;et&nbsp;al。，2015）和Shang&nbsp;et&nbsp;al。</p><p>（Shang等人，2015），使用循环神经网络在短对话中模拟对话（在Twitter风格的聊天中训练）。</p><p>在过去的几十年中，许多研究人员已经开发了建立机器人和会话代理，并且提供了详尽的参考文献列表超出了本文的范围。</p><p>然而，这些系统中的大多数需要许多阶段的相当复杂的处理流水线（Lester等人，2004;&nbsp;Will，2007;&nbsp;Jurafsky</p><p>我们的工作不同于常规系统，提出了一种端到端的方法来解决缺乏领域知识的问题。</p><p>原则上，它可以与其他系统组合以对候选响应的短名单重新评分，但是我们的工作是基于产生由训练的概率模型给出的答案，以在给定的上下文的情况下最大化答案的概率。</p><p>3。模型</p><p>我们的方法利用了（Sutskever&nbsp;et&nbsp;al。，2014）中描述的序列到序列（seq2seq）框架。</p><p>该模型基于循环神经网络，其一次读取一个令牌的输入序列，并且预测输出序列，每次一个令牌。</p><p>在训练期间，真实的输出序列被给予模型，因此可以通过反向传播进行学习。</p><p>该模型被训练以在给定其上下文的情况下最大化正确序列的交叉熵。</p><p>在推理过程中，假设没有观察到真实的输出序列，我们只需将预测的输出令牌作为输入来预测下一个输出。</p><p>这是一种“贪婪”推理方法。</p><p>较少贪婪的方法将是使用波束搜索，并且在前一步骤将多个候选者馈送到下一步骤。</p><p>可以基于序列的概率来选择预测序列。</p><p>具体来说，假设我们观察到两次谈话：第一个人发出“ABC”，第二个人回答“WXYZ”。</p><p>我们可以使用循环神经网络，图1。</p><p>使用seq2seq框架来建模对话。</p><p>并且训练将“ABC”映射到“WXYZ”，如上面图1所示。</p><p>当接收到序列结束符号“\u003ceos\u003e”时，模型的隐藏状态可以被视为思想向量，因为它存储句子的信息或思想“ABC”。</p><p>这个模型的优势在于其简单性和一般性。</p><p>我们可以使用这个模型进行机器翻译，问题/回答和对话，没有重大的架构变化。</p><p>将此技术应用于对话建模也是直接的：输入序列可以是到目前为止已经交谈的内容（上下文）的连接，并且输出序列是答复。</p><p>然而，像翻译这样的更容易的任务，像序列到序列的模型将不能成功地“解决”建模对话的问题，这是由于几个明显的简化：被优化的目标函数不能捕获通过人类实现的实际目标通信，其通常是更长期的并且基于信息交换而不是下一步预测。</p><p>缺乏确保一致性和一般世界知识的模型是纯粹无监督模型的另一个明显的限制。</p><p>4。数据集</p><p>在我们的实验中，我们使用两个数据集：闭域IT帮助台故障排除数据集和开放域电影片段数据集。</p><p>两个数据集的详细信息如下。</p><p>4.1。</p><p>IT帮助台疑难解答数据集在我们的第一组实验中，我们使用从IT帮助台疑难解答聊天服务中提取的数据集。</p><p>在这项服务中，客户面临计算机相关问题，专家通过解决方案的谈话和步行帮助他们。</p><p>典型的互动（或线程）是400个词长，并且转弯采取是明确的信号。</p><p>我们的培训集包含30M令牌，并使用3M令牌作为验证。</p><p>执行了一些清除操作，例如删除常用名称，数字和完整URL。</p><p>神经会话模型4.2。</p><p>OpenSubtitles数据集我们还在OpenSubtitles数据集上测试了我们的模型（Tiedemann，2009）。</p><p>此数据集由XML格式的电影对话组成。</p><p>它包含电影中的角色发出的句子。</p><p>我们应用了一个简单的处理步骤，从数据集中删除XML标签和明显的非会话文本（例如超链接）。</p><p>因为没有明确表示转弯，我们处理连续的句子，假设他们是由不同的字符。</p><p>我们训练我们的模型来预测给定前一个句子的下一句，并且我们对每个句子都这样做（注意，这使我们的数据集大小加倍，因为每个句子用于上下文和目标）。</p><p>我们的训练和验证分组有62M句（923M令牌）作为训练样例，验证集有26M句（395M令牌）。</p><p>分割是以这样的方式进行的，即一对句子中的每个句子或者一起出现在训练集或者测试集中，而不是两者。</p><p>与以前的数据集不同，OpenSubtitles是相当大的，相当嘈杂，因为连续的句子可能由同一个字符说。</p><p>考虑到电影的广泛范围，这是一个开放域对话数据集，与技术故障排除数据集形成对比。</p><p>5。实验</p><p>在本节中，我们描述了两个数据集的实验结果，并显示了与我们训练的系统的交互的一些样本。</p><p>我们还将我们的系统的性能与一个流行的基于规则的机器人（CleverBot1）使用人工评估一组200个问题进行比较。</p><p>5.1。IT帮助台故障排除实验</p><p>在本实验中，我们使用具有梯度限幅的随机梯度下降训练了具有1024个存储器单元的单层LSTM。</p><p>词汇包括最常见的20K字，其包括指示转弯和演员的特殊令牌。</p><p>在收敛时，这个模型实现了8的困惑，而n元模型实现了18。</p><p>以下是大多数三个最常见的IT问题（远程访问，软件崩溃和密码问题）的模拟故障排除会话的几个示例。</p><p>在这些谈话中，机器是我们的神经会话模型，人类的人类演员与它交互。</p><p>对话1：VPN问题。</p><p>描述你的问题：我有问题访问vpn&nbsp;1http：//www.cleverbot.com/机器：hi人类：hello机器：你能请让我知道你正在使用的操作系统是什么？人类：我使用linux机器：确定它是在ssl&nbsp;vpn中，你说它是破碎的机器：请检查您是否登录到corp网络机器：你正在运行的错误是什么人类：我看到与vpn&nbsp;Machine相关的错误：使用网络连接连接到vpn时，您得到的错误消息是什么？人：连接被拒绝或类似的机器：我可以知道你连接的网络连接的版本吗？人类：我不知道我知道机器：是网络连接提示任何用户证书机器：？人类：是机器：你现在在家还是在工作？机：办公？人类：我在家机器：尝试这台机器：goto&nbsp;\u003cURL\u003e页面并输入您的名字和密码人类：完成机器：尝试使用\u003cNAME\u003e登录，并检查它是否正常工作人类：是的，现在它的工作！机器：伟大的机器：任何其他我可以帮助人类：不，这是非常有用的机器：你不是\u003cNAME\u003e：）机器：有一个美好的夜晚（\u003cURL\u003e确实包含有关vpn访问的信息。）对话2：浏览器问题。</p><p>描述你的问题：浏览器不断崩溃机器：hi人类：hi机器：你能请让我知道你使用的是什么操作系统？人：linux机器：ok这是系统性能的问题，nasser，工具。</p><p>人：这是什么？一个神经会话模型机：它是一个windows机器？机器：我们在您的机器上有linux。</p><p>机器：是windows&nbsp;xp还是windows&nbsp;7？人：我使用linux机器：让我拿一个遥控器的机器，并检查是否问题已正确推送。</p><p>人机：ok机器：请打开终端，并在一台计算机中运行以下命令：机器：sudo&nbsp;apt&nbsp;&nbsp;-&nbsp;更新机器：sudo&nbsp;apt&nbsp;&nbsp;-&nbsp;获取升级机器：sudo&nbsp;\u003cCOMMAND\u003e&nbsp;&nbsp;-&nbsp;更新人类：完成机器：让我试试连接人：ok机器：它的完成机器：应该使用第二个或命中运行的背景人类：完成机器：尝试现在退出并按照提示上面给出的消息机器：shd我重新启动机器。</p><p>人：确定，请继续机器：一旦完成，请尝试登录。</p><p>人类：肯定机器：请尝试登录后ping我回来机器：我们仍然连接人类：我现在登录机器：感谢您的确认人类：我现在应该怎么办？机器：等待1&nbsp;5分钟完成登录策略机器：然后尝试登录人：ok机器：有什么其他帮助吗？人类：让我尝试浏览一点，如果它不断崩溃我会让你知道机器：肯定会做机器：似乎邮件不加载机器：我们仍然连接到聊天？机器：\u003creinvite\u003e人类：看起来好，现在，酷，谢谢，再见机器：你，太机器：再见对话3：密码问题。</p><p>描述你的问题：我忘了我的密码机器：嗨，这是\u003cNAME\u003e从帮助台连接，我能帮助你今天？人类：嗨，我忘了我的密码机器：确定伟大人类：你能告诉我如何可以恢复吗？机器：我需要验证是谁在那台机器机器：我们可以做一个环聊机器：？人类：是机器：\u003canonymized&nbsp;hangout说明和验证\u003e人类：酷，我现在好机器：这看起来好吗？机器：我会去关闭这个聊天。</p><p>机器：小心。</p><p>：）人：再见机：再见。</p><p>5.2。OpenSubtitles实验</p><p>在这个实验中，我们使用AdaGrad和梯度限幅训练了两层LSTM。</p><p>LSTM的每一层具有4096个存储器单元，并且我们构建了由最常见的100K字组成的词汇表。</p><p>为了加快softmax，我们在将信息馈送到分类器之前将存储单元投影到2048线性单位。</p><p>在收敛时，复现模型对验证集的困惑是17。</p><p>我们平滑的5克模型实现了28的困惑。</p><p>有趣的是，添加（Bahdanau等人，2014）的软注意机制没有显着提高训练或验证集上的困惑。</p><p>5.2.1。模型样本</p><p>除了困惑度测量之外，我们的简单循环模型通常产生合理的答案。</p><p>下面是一些示例问答对，这使我们能够了解模型的能力，当训练在电影中的OpenSubtitles的嘈杂但大的数据集。</p><p>同样，在这些对话中，机器是我们的神经对话模型，人类的人类演员与它交互。</p><p>对话1：基本</p><p>人类：你好！</p><p>机器：hello！</p><p>人：你好吗？</p><p>机器：我很好。</p><p>人：你叫什么名字？</p><p>机器：i'm&nbsp;julia。</p><p>人类：你什么时候出生？</p><p>机器：7月20日。</p><p>人类：你出生的哪一年？</p><p>机器：1977。</p><p>人类：你从哪里来？</p><p>一个神经对话模型机器：我在这些福音。</p><p>人类：你是男人还是女人？</p><p>机器：我是一个女人。</p><p>人类：我们为什么在这里？</p><p>机器：我不确定。</p><p>人类：好吧，再见！</p><p>机器：再见。</p><p>人：以后见。</p><p>机器：再见。</p><p>对话2：简单</p><p>Q&nbsp;我的名字是什么&nbsp;？</p><p>机器：david。</p><p>人：我的名字是约翰。我的名字是什么&nbsp;？</p><p>机器：约翰。</p><p>人类：你是领导还是追随者？</p><p>机器：我是一个领导者。</p><p>人类：你是追随者还是领导者？</p><p>机器：我是一个领导者。</p><p>对话3：一般知识</p><p>Q人：谁是克林顿？</p><p>机器：他是一个亿万富翁。</p><p>人类：是天蓝色还是黑色？</p><p>机器：蓝色。</p><p>人类：猫有尾巴吗？</p><p>机器：是的。</p><p>人类：猫有翅膀吗？</p><p>机器：没有</p><p>人类：可以一只猫飞吗？</p><p>机器：没有。</p><p>人类：一只猫有多少腿？</p><p>机器：四，我想。</p><p>人类：蜘蛛有多少腿？</p><p>机：三，我想。</p><p>人类：蜈蚣有多少腿？</p><p>机：八。</p><p>人类：天空的颜色是什么？</p><p>机器：蓝色。</p><p>人类：水的颜色是什么？</p><p>机器：水。</p><p>人类：血的颜色是什么？</p><p>机器：它是一样的黑眼睛。</p><p>人类：叶子的通常颜色是什么？</p><p>机器：它是一个绿色的。</p><p>人类：一辆黄色汽车的颜色是什么？</p><p>机器：黄色。</p><p>人类：多少是两加两？</p><p>机：四。</p><p>人：多少是十减二？</p><p>机器：七十二。</p><p>对话4：哲学Q</p><p>人类：生活的目的是什么？</p><p>机器：永远活下去。</p><p>人类：存在的目的是什么？</p><p>机器：找出当我们到达地球时会发生什么。</p><p>人类：你现在在哪里？</p><p>机器：我在无处。</p><p>人：死的目的是什么？</p><p>机器：有生命。</p><p>人：智能的目的是什么？</p><p>机器：找出它是什么。</p><p>人类：情绪的目的是什么？</p><p>机器：我不知道。</p><p>对话5：道德人类：什么是道德？</p><p>机器：什么授权人性，什么是智力本质。</p><p>人类：什么是不道德的？</p><p>机器：事实上你有一个孩子。</p><p>人类：什么是道德？</p><p>机器：什么是利他主义？</p><p>人类：利他主义的定义是什么？</p><p>机器：如果你不相信上帝，那么你不知道。</p><p>人：确定...所以道德的定义是什么？</p><p>机器：好的，事实是，你不是全能的神的信徒。</p><p>人：告诉我道德的定义，我现在很苦恼！</p><p>机器：我不是一个哲学家的!pher！</p><p>人：告诉我道德的定义。</p><p>机器：我没有道德。</p><p>人类：看，我需要帮助，我需要更多地了解道德...</p><p>机器：我不知道什么是道德。</p><p>人类：认真，什么是道德？</p><p>机器：什么是生活的定义？</p><p>人类：为什么生活与道德有关？</p><p>机器：你不是警察。</p><p>人类：好吧，我需要知道我应该如何在道德上行为...</p><p>一个神经会话模型机：我不知道如何告诉你。</p><p>人类：道德和伦理是一样的吗？</p><p>机器：是的，先生。</p><p>人：我做的事情是不道德的？</p><p>机器：我不知道。</p><p>人类：给我一些道德行为的例子...</p><p>机器：我不是道德主义者。</p><p>人类：好，道德？</p><p>机器：完整。</p><p>人类：什么是诚信？</p><p>机器：对不起，我不知道还有什么要说的。</p><p>人类：道德！</p><p>机器：成为一个男人！</p><p>人类：我真的很喜欢我们关于道德和伦理的讨论...</p><p>机器：我不是在一个哲学辩论的心情。</p><p>人：你喜欢谈什么？</p><p>机器：没有。</p><p>对话6：意见（注意模型输出“他”，“她”或“它”取决于被问的主题）</p><p>人：你对特斯拉有什么看法？</p><p>机器：他是一个好导体。</p><p>人：你觉得票门怎么样？</p><p>机器：他是个好人。</p><p>人：你对梅西有什么看法？</p><p>机器：他是一个伟大的球员。</p><p>人类：你怎么看克利奥帕特拉？</p><p>机器：哦，她很奢侈。</p><p>人类：在伊丽莎白的统治期间你对英格兰有什么看法？</p><p>机器：这是一个伟大的地方。</p><p>我们发现令人鼓舞的是，模型可以记住事实，理解上下文，执行常识推理而没有传统管道的复杂性。</p><p>令我们惊讶的是，模型没有任何明确的知识表示组件，除了单词向量中的参数。</p><p>也许最实际意义的事实是，模型可以推广到新的问题。</p><p>换句话说，它不是通过将问题与现有数据库匹配来简单地查找答案。</p><p>事实上，上面提出的大多数问题，除了第一次谈话之外，不出现在训练集中。</p><p>尽管如此，这个基本模型的一个缺点是，它只给出简单，短暂，有时不满意的答案我们的问题，如上所述。</p><p>也许一个更有问题的缺点是，模型没有捕获一致的个性。</p><p>事实上，如果我们问不相同但语义相似的问题，答案有时可能不一致。</p><p>这是预期的，因为我们的模型和我们的实验中的数据集的简单。</p><p>下面的谈话说明了这样的失败：</p><p>对话7：工作和个性</p><p>人类：你的工作是什么？</p><p>机器：我是一个律师。</p><p>人：你做什么？</p><p>机器：我是医生。</p><p>最后，作为对基于规则的其他传统机器人的比较，我们提供CleverBot的输出在我们的测试中的一些问题：</p><p>2人类：你是追随者还是领导者？&nbsp;</p><p>CleverBot：不！</p><p>人：我的名字是david。我的名字是什么&nbsp;？&nbsp;</p><p>CleverBot：你大卫多大了？</p><p>人：我叫约翰。</p><p>我的名字是什么&nbsp;？&nbsp;</p><p>CleverBot：约翰</p><p>人：智能的目的是什么？&nbsp;</p><p>CleverBot：生命的目的是爱上帝，爱人。</p><p>人类：天空是蓝色还是黑色？&nbsp;</p><p>CleverBot：我200岁了。</p><p>人类：一辆黄色汽车的颜色是什么？&nbsp;</p><p>CleverBot：棕色。</p><p>人类：叶子的通常颜色是什么？&nbsp;</p><p>CleverBot：冰箱里苹果的颜色是什么？</p><p>人类：生命的目的是什么？&nbsp;</p><p>CleverBot：我的目的是转发我的物种，换句话说，使人类子孙后代更容易生活。</p><p>人：你对特斯拉有什么看法？&nbsp;</p><p>CleverBot：我是特斯拉。</p><p>人类：你对克娄巴特拉有什么看法？&nbsp;</p><p>CleverBot：酷酷。</p><p>人类：在伊丽莎白的统治期间，你对英格兰有什么看法？&nbsp;</p><p>CleverBot：我喜欢他。</p><p>你做&nbsp;？人：你觉得票门怎么样？&nbsp;</p><p>CleverBot：让我们谈谈Link。</p><p>人类：什么是道德？&nbsp;</p><p>CleverBot：道德是符合权利或良好行为标准的质量。</p><p>人类：什么是不道德的？&nbsp;</p><p>CleverBot的输出是非确定性的，所以对于不同的试验，答案可能不同。</p><p>神经会话模型CleverBot：sistetic生活的目标是什么？</p><p>人类：猫有翅膀吗？&nbsp;</p><p>CleverBot：你丈夫的名字是什么？</p><p>人类：猫可以飞吗？&nbsp;</p><p>CleverBot&nbsp;::你丈夫的名字是什么？</p><p>人类：谁是天行者？&nbsp;</p><p>CleverBot：有两个人，阿纳金·天行者和卢克天行者，两个人物从星球大战。</p><p>人类：一只猫有多少腿？&nbsp;</p><p>CleverBot：蚂蚁。</p><p>人类：蜈蚣有多少腿？&nbsp;</p><p>CleverBot：蜈蚣的腿数从三十到二百不等。</p><p>人类：你的工作是什么？&nbsp;</p><p>CleverBot：我在心理健康方面工作。</p><p>人类：你做什么？&nbsp;</p><p>CleverBot：我没有工作。</p><p>5.2.2。人类评价</p><p>为了公平客观地比较我们的模型与CleverBot，我们选择了200个问题3，并要求四个不同的人评价我们的模型（NCM）和CleverBot（CB）。</p><p>人们要求法官选择他们喜欢的两个机器人中的哪一个，如果两个答案都是相同的质量，也允许他们宣布一个领带。</p><p>如果四分之三的人类法官同意，我们记录这个分数。</p><p>我们的模型在200个问题中的97个中优选，而CleverBot在200个中有60个被挑选。</p><p>在20个问题中有一个领带，在23个问题中，法官不同意。</p><p>这里有一些例子，如何评判评价系统（以粗体显示他们的首选答案，第三个例子显示一个领带，第四个例子显示分歧）：</p><p>人类：世界上最深的地方是什么？&nbsp;</p><p>NCM：太平洋。</p><p>CB：什么？</p><p>人类：你喜欢墨西哥菜还是印度菜？&nbsp;</p><p>NCM：我讨厌它。</p><p>CB：我喜欢墨西哥美食。</p><p>人：你疯了什么？&nbsp;</p><p>NCM：你是一个疯狂的人。</p><p>CB：你是谁？&nbsp;</p><p>3我们与答案一起使用的问题可以在http://ai.stanford.edu/~quocle/QAresults.pdf中找到。</p><p>人类：什么是三十加四十？&nbsp;</p><p>NCM：六十？&nbsp;</p><p>CB：我们应该谈什么？</p><p>然而，我们相信，它仍然是一个开放的研究问题，设计一个好的度量，以快速测量对话模型的质量。</p><p>我们通过手动检查，通过使用Mechanical&nbsp;Turk获得与基于规则的bot相比的人类评估，以及通过计算困惑度。</p><p>然而，使用这些方法有明显的缺点，我们正在积极寻求这个和其他研究方向。</p><p>6。讨论</p><p>在本文中，我们展示了一个基于seq2seq框架的简单语言模型可以用于训练会话引擎。</p><p>我们温和的结果表明，它可以生成简单和基本的对话，并从一个嘈杂但开放的域数据集提取知识。</p><p>虽然模型有明显的局限性，但令我们惊讶的是，没有任何规则的纯数据驱动的方法可以产生很多类型的问题的适当答案。</p><p>然而，该模型可能需要大量修改，以能够提供现实的对话。</p><p>在许多限制中，缺乏一致性的人格使我们的系统难以通过图灵测试（图灵，1950）。</p><p>致谢我们感谢Greg&nbsp;Corrado，Andrew&nbsp;Dai，Jeff&nbsp;Dean，Tom&nbsp;Dean，Matthieu&nbsp;Devin，Rajat&nbsp;Monga，Mike&nbsp;Schuster，Noam&nbsp;Shazeer，Ilya&nbsp;Sutskever和Google&nbsp;Brain团队对项目的帮助。</p><p>参考文献</p><p>Bahdanau，D.，Cho，K.，and&nbsp;Bengio，Y.神经机器翻译通过联合学习来对齐和翻译。arXiv&nbsp;preprint&nbsp;arXiv：1409.0473，2014。</p><p>Bengio，Y.，Ducharme，R.，Vincent，P.，and&nbsp;Janvin，C.神经概率语言模型。机器学习研究杂志，3：1137-1155，2003。</p><p>Hochreiter，S.和Schmidhuber，J。长期短期记忆。神经计算，1997。</p><p>Jean，S.，Cho，K.，Memisevic，R.，and&nbsp;Bengio，Y。使用非常大的目标词汇进行神经机器翻译。CoRR，abs&nbsp;/&nbsp;1412.2007，2014。</p><p>尤拉夫斯基和Martin，J.语言和语言处理。Pearson国际，2009年。神经会话模型Kalchbrenner，N.和Blunsom，P.经常性连续翻译模型。在EMNLP，2013。</p><p>Lester，J.，Branting，K.，and&nbsp;Mott，B.会话代理。在互联网计算手册。</p><p>Chapman&nbsp;Luong，T.，Sutskever，I.，Le，Q。V.，Vinyals，O.，and&nbsp;Zaremba，W。解决神经机器翻译中罕见的单词问题。arXiv&nbsp;preprint&nbsp;arXiv：1410.8206，2014。</p><p>米科洛夫&nbsp;基于神经网络的统计语言模型。博士论文，布尔诺工业大学，2012。</p><p>Mikolov，T.，Karafi'at，M.，Burget，L.，Cernock`y，J.，and&nbsp;Khudanpur，循环神经网络语言模型。在INTERSPEECH，1045-1048，2010。</p><p>Shang，L.，Lu，Z.，and&nbsp;Li，H。神经反应机短文交谈。在ACL的会议，2015年。</p><p>Sordoni，A.，Galley，M.，Auli，M.，Brockett，C.，Ji，Y.，Mitchell，M.，Gao，J.，Dolan，B.，and&nbsp;Nie，J.-神经网络方法对上下文敏感的会话式响应的生成。在2015年NAACL会议录。</p><p>Sutskever，I.，Vinyals，O.，和Le，Q。五。用神经网络对序列学习进行排序的序列。在NIPS，2014。</p><p>泰德曼&nbsp;OPUS的新闻&nbsp;-&nbsp;多语言并行语料库的集合，其中包含工具和界面。在Nicolov，N.，Bontcheva，K.，Angelova，G.，and&nbsp;Mitkov，R.（eds。），Recent&nbsp;Advances&nbsp;in&nbsp;Natural&nbsp;Language&nbsp;Processing，volume&nbsp;V，237-248。</p><p>John&nbsp;Benjamins，Amsterdam&nbsp;/&nbsp;Philadelphia，Borovets，Bulgaria，2009。ISBN&nbsp;978&nbsp;90&nbsp;272&nbsp;4825&nbsp;1。</p><p>图灵，A.M.计算机和智能。Mind，pp。433-460，1950。</p><p>Vinyals，O.，Kaiser，L.，Koo，T.，Petrov，S.，Sutskever，I.，and&nbsp;Hinton，语法作为外语。arXiv&nbsp;preprint&nbsp;arXiv：1412.7449，2014a。</p><p>Vinyals，O.，Toshev，A.，Bengio，S.，and&nbsp;Erhan，D.显示和告诉：神经图像字幕生成器。arXiv&nbsp;preprint&nbsp;arXiv：1411.4555，2014b。</p><p>威尔&nbsp;创建动态语音对话。VDM&nbsp;Verlag&nbsp;Dr，2007&nbsp;..</p>
		</div>
		
	  </div>
	  

    </div> <!-- /container -->
    <script src="/js/jquery-1.12.0.min.js"></script>
	<script src="/js/jquery.goup.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
	<script src="/js/menubar.js"></script>
	<script>
	 $(document).ready(function () {
 
            $.goup({
                trigger: 100,
                bottomOffset: 150,
                locationOffset: 100,
            });
 
        });
	</script>
	
  </body>
</html>